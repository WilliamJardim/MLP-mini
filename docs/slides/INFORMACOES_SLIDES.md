# Como se calcula os gradientes das unidades usando o backpropagation
![Slide Backpropagation](./Slide_Backpropagation_APRIMORADO.png "Backpropagation")

Esta imagem foi criada como parte do meu aprendizado sobre algoritmos de propagação direta e Backpropagation em redes neurais. Ela reflete meu entendimento pessoal.

Nela mostra o processo fundamental do Backpropagation: **Para se calcular o gradiente de uma unidade que está na camada oculta, nós vamos precisar somar os gradientes de todas as unidades da camada seguinte multiplicados pelo peso que faz essa conexão entre a unidade em questão na camada oculta e a unidade atual da camada seguinte.** Em outras palavras, esse calculo que fazemos é um processo repetitivo e muito simples, que se resume em: Para cada unidade na camada seguinte, some o gradiente dessa unidade(isso é, a unidade da camada seguinte) em questão multiplicado pelo peso(do vetor de pesos) dessa unidade(isso é, a unidade da camada seguinte) em questão cujo indice é O NÙMERO DA UNIDADE ATUAL DA CAMADA OCULTA, ou seja, o peso de número <NÙMERO_DA_UNIDADE_ATUAL>. Esse peso é usado porque esse é o peso(do vetor de pesos) da unidade da camada seguinte, [peso este] que multiplica a saida da unidade em questão da camada oculta, que é usado na etapa de Feedforward. Ou seja, as unidades da camada seguinte na etapa de Feedforward fazem uso desses pesos para propagar os valores para frente, conforme descrito, e fazem isso multiplicando a saida das unidades da camada anterior por seu respectivo peso que é responsável por criar esse eloh, e então, no Backpropagation nós fazemos o caminho reverso/contrário. Ou seja, para calcular o gradiente de uma determinada unidade na camada oculta, vamos ter que usar os gradientes e também os pesos **de todas** as unidades da camada seguinte, isso por que os gradientes das unidades da camada oculta atual dependem dos gradientes das unidades da camada seguinte, e também depdendem dos pesos da camada seguinte, conforme explicado aqui nesse texto, e também ilustrado na imagem. 

Então esse processo é um somatório, e **se resume em: "GRADIENTE * PESO (+) GRADIENTE * PESO (+) ETC...."**, eu coloquei "ETC...." por que essa soma continuaria até que chegasse na ultima unidade da camada seguinte. Reforçando esse ponto, que para calcular o gradiente de uma unidade na camada oculta atual estamos somando os gradientes das unidades da camada seguinte(ou seja, a camada que está a frente da camada oculta atual). E aqui usei o termo "camada oculta atual" por que esse processo é sempre o mesmo para toda e qualquer camada oculta, sempre da mesma forma, ou seja, sempre usando os gradientes da camada seguinte, e também os pesos, conforme ilustrado na imagem, e muito bem descrito aqui. É muito facil de memorizar mentalmente esse fluxo de operações!